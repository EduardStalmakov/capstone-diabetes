{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"456ef991-02da-49bd-96ff-82b62cea683c","showTitle":false,"title":""}},"source":["# `NHAINESStat` Populator"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f8489d55-6da5-4116-88c4-875e2a3388b9","showTitle":false,"title":""}},"source":["## Step 1: Set-Up"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"44a6c2e5-de16-4d78-8ea7-d8eb7e49afa7","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Imports\n","import uuid\n","from confluent_kafka.admin import AdminClient, NewTopic\n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import lit\n","from pyspark.sql.types import FloatType\n","from pyspark.sql.types import StringType\n","import pandas as pd\n","\n","# Get config\n","from config import user\n","from config import password"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aa5e1160-1589-49b7-a9de-5014c78b0f57","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">/mnt/jacklynn/nhaines has been unmounted.\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">/mnt/jacklynn/nhaines has been unmounted.\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Mount point through Oauth security.\n","storageAccount = \"gen10datafund2205\"\n","storageContainer = \"group5container\"\n","clientSecret = \"-ZS8Q~NwOKfwEpVOg3Teb1pPtxDbz616XjlXLbuU\"\n","clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n","mount_point = \"/mnt/jacklynn/nhaines\" \n","\n","# Configuration dictionary\n","configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n","       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n","       \"fs.azure.account.oauth2.client.id\": clientid,\n","       \"fs.azure.account.oauth2.client.secret\": clientSecret,\n","       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n","       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n","\n","# Unmount if exists\n","try: \n","    dbutils.fs.unmount(mount_point)\n","except:\n","    pass\n","\n","# Mount to database\n","dbutils.fs.mount(\n","    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n","    mount_point = mount_point,\n","    extra_configs = configs)\n","\n","# Table variables\n","database = \"group5database\"\n","server = \"gen10-data-fundamentals-22-05-sql-server.database.windows.net\"\n","port = \"1433\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3cb0454a-5c37-443d-a073-262f0367f71c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/jacklynn/nhaines/CGM_Data.csv</td><td>CGM_Data.csv</td><td>35978185</td><td>1659468631000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/Diabetes Prevalence in the US by State and Demographic.csv</td><td>Diabetes Prevalence in the US by State and Demographic.csv</td><td>180068</td><td>1659497875000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/Education by state.csv</td><td>Education by state.csv</td><td>3516</td><td>1659576860000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/ExerciseData_2013_150min.csv</td><td>ExerciseData_2013_150min.csv</td><td>1544</td><td>1659645073000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/Food Insecurity.csv</td><td>Food Insecurity.csv</td><td>6779</td><td>1659533925000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/Income Brackets by State.csv</td><td>Income Brackets by State.csv</td><td>4675</td><td>1659578726000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/U.S. NHANES Survey Data.csv</td><td>U.S. NHANES Survey Data.csv</td><td>1005266</td><td>1659710519000</td></tr><tr><td>dbfs:/mnt/jacklynn/nhaines/chinese-diabetes-clean.csv</td><td>chinese-diabetes-clean.csv</td><td>33367142</td><td>1659541878000</td></tr></tbody></table></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"aggData":[],"aggError":"","aggOverflow":false,"aggSchema":[],"aggSeriesLimitReached":false,"aggType":"","arguments":{},"columnCustomDisplayInfos":{},"data":[["dbfs:/mnt/jacklynn/nhaines/CGM_Data.csv","CGM_Data.csv",35978185,1659468631000],["dbfs:/mnt/jacklynn/nhaines/Diabetes Prevalence in the US by State and Demographic.csv","Diabetes Prevalence in the US by State and Demographic.csv",180068,1659497875000],["dbfs:/mnt/jacklynn/nhaines/Education by state.csv","Education by state.csv",3516,1659576860000],["dbfs:/mnt/jacklynn/nhaines/ExerciseData_2013_150min.csv","ExerciseData_2013_150min.csv",1544,1659645073000],["dbfs:/mnt/jacklynn/nhaines/Food Insecurity.csv","Food Insecurity.csv",6779,1659533925000],["dbfs:/mnt/jacklynn/nhaines/Income Brackets by State.csv","Income Brackets by State.csv",4675,1659578726000],["dbfs:/mnt/jacklynn/nhaines/U.S. NHANES Survey Data.csv","U.S. NHANES Survey Data.csv",1005266,1659710519000],["dbfs:/mnt/jacklynn/nhaines/chinese-diabetes-clean.csv","chinese-diabetes-clean.csv",33367142,1659541878000]],"datasetInfos":[],"dbfsResultPath":null,"isJsonSchema":true,"metadata":{"isDbfsCommandResult":false},"overflow":false,"plotOptions":{"customPlotOptions":{},"displayType":"table","pivotAggregation":null,"pivotColumns":null,"xColumns":null,"yColumns":null},"removedWidgets":[],"schema":[{"metadata":"{}","name":"path","type":"\"string\""},{"metadata":"{}","name":"name","type":"\"string\""},{"metadata":"{}","name":"size","type":"\"long\""},{"metadata":"{}","name":"modificationTime","type":"\"long\""}],"type":"table"}},"output_type":"display_data"}],"source":["%fs \n","ls /mnt/jacklynn/nhaines"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fde70c6b-00a9-4560-a521-d58a5caee82e","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Helper function: read in table\n","def readInTable(table_name):\n","    df = spark.read.format(\"jdbc\") \\\n","        .option(\"url\", f\"jdbc:sqlserver://{server}:{port};databaseName={database};\") \\\n","        .option(\"dbtable\", table_name).option(\"user\", user).option(\"password\", password) \\\n","        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n","        .load()\n","    return df\n","\n","# Helper function: read in file\n","def readInFile(f):\n","    df = spark.read.options(\n","        inferSchema='True',\n","        delimiter=',',\n","        header='True'\n","        ).csv(f)\n","    return df\n","\n","# Helper function: write in table\n","def saveToTable(df, table, change='append'):\n","    df.write.format('jdbc').option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n","                .mode(change) \\\n","                .option(\"dbtable\", table) \\\n","                .option(\"user\", user) \\\n","                .option(\"password\", password) \\\n","                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n","                .save()\n","\n","# Helper function: convert table into dictionary converter\n","def formDictConverter(table, key, value):\n","    df = readInTable(table)\n","    converter = dict()\n","    data = df.select([key, value]).distinct().toPandas()[[key, value]]\n","    keys = data[key].to_list()\n","    for this_key in keys:\n","        converter[this_key] = data.loc[data[key] == this_key][value].to_list()[0]\n","    return converter"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"73acbf77-f310-4290-af3c-f9a0cca62939","showTitle":false,"title":""}},"source":["## Part 2: Populate `Demographic` Database"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4f1f3bd0-03a0-4012-8760-8297a2a04bdc","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Get starting demoID\n","table_demo = \"dbo.Demographic\"\n","df_demo = readInTable(table_demo)\n","j = df_demo.agg({\"demoID\": \"max\"}).collect()[0]['max(demoID)'] + 1\n","\n","# Write function for adding states data\n","def addDemographics():\n","    \n","    # Useful variable\n","    schema = ['demoID', 'demo_group', 'category']\n","    table_demo = \"dbo.Demographic\"\n","    \n","    # Helper function\n","    \n","    # Helper function: create category\n","    def createCategory(group, category):\n","        global j\n","        demoID = j\n","        j += 1\n","        return [demoID, group, category]\n","    \n","    # Helper function: remove duplicates\n","    def dedup(starting_list):\n","        used_list = readInTable(\"dbo.Demographic\").select(['demo_group']).toPandas()['demo_group'].to_list()\n","        for demo in starting_list:\n","            if demo in used_list or demo.capitalize() in used_list or demo.lower() in used_list:\n","                starting_list.remove(demo)\n","        return starting_list\n","    \n","    # Get diabetes prevalence demographics\n","    df_nhaines = readInFile('/mnt/jacklynn/nhaines/U.S. NHANES Survey Data.csv')\n","    df_nhaines = df_nhaines.select(['AdultEducationLevel', 'AnnualHouseholdIncome', 'Ethnicity'])\n","    \n","    # Get drinking demos\n","    drinking_demos = ['12+ alc. drinks/yr', '<12 alc. drinks/yr']\n","    drinking_demos = dedup(drinking_demos)\n","    \n","    # Only if values exist save\n","    if (len(drinking_demos) != 0):\n","    \n","        # Add all of demos under 'race/ethnicity' category\n","        drinking_demos_map = map(lambda x: createCategory(x, 'drinking (NHAINES)'), drinking_demos)\n","        df_drinking_demos = spark.createDataFrame(data = drinking_demos_map, schema = schema)\n","\n","        # Save to table\n","        saveToTable(df_drinking_demos, table_demo)\n","    \n","    # Get smoking demos\n","    smoking_demos = ['Smoked 100+ cigs.', 'Smoked <100 cigs.']\n","    smoking_demos = dedup(smoking_demos)\n","    \n","    # Only if values exist save\n","    if (len(smoking_demos) != 0):\n","    \n","        # Add all of demos under 'race/ethnicity' category\n","        smoking_demos_map = map(lambda x: createCategory(x, 'smoking (NHAINES)'), smoking_demos)\n","        df_smoking_demos = spark.createDataFrame(data = smoking_demos_map, schema = schema)\n","\n","        # Save to table\n","        saveToTable(df_smoking_demos, table_demo)\n","    \n","    # Get education demos\n","    \n","    # Get categories\n","    education_demos = df_nhaines.select('AdultEducationLevel').distinct().toPandas()['AdultEducationLevel'].to_list()\n","    \n","    # Iterate through each one and remove those that are already in list\n","    education_demos = dedup(education_demos)\n","    \n","    # Only if values exist save\n","    if (len(education_demos) != 0):\n","    \n","        # Add all of demos under 'race/ethnicity' category\n","        education_demos_map = map(lambda x: createCategory(x, 'education level'), education_demos)\n","        df_education_demos = spark.createDataFrame(data = education_demos_map, schema = schema)\n","\n","        # Save to table\n","        saveToTable(df_education_demos, table_demo)\n","    \n","    # Get income demos\n","    \n","    # Get categories\n","    income_demos = df_nhaines.select('AnnualHouseholdIncome').distinct().toPandas()['AnnualHouseholdIncome'].to_list()\n","    \n","    # Iterate through each one and remove those that are already in list\n","    income_demos = dedup(income_demos)\n","    \n","    # Only if values exist save\n","    if (len(income_demos) != 0):\n","    \n","        # Add all of demos under 'race/ethnicity' category\n","        income_demos_map = map(lambda x: createCategory(x, 'income bracket'), income_demos)\n","        df_income_demos = spark.createDataFrame(data = income_demos_map, schema = schema)\n","\n","        # Save to table\n","        saveToTable(df_income_demos, table_demo)\n","        \n","    # Get income demos\n","    \n","    # Get categories\n","    ethinicity_demos = df_nhaines.select('Ethnicity').distinct().toPandas()['Ethnicity'].to_list()\n","    \n","    # Iterate through each one and remove those that are already in list\n","    ethinicity_demos = dedup(ethinicity_demos)\n","    \n","    # Only if values exist save\n","    if (len(ethinicity_demos) != 0):\n","    \n","        # Add all of demos under 'race/ethnicity' category\n","        ethinicity_demos_map = map(lambda x: createCategory(x, 'race/ethnicity'), ethinicity_demos)\n","        df_ethnicity_demos = spark.createDataFrame(data = ethinicity_demos_map, schema = schema)\n","\n","        # Save to table\n","        saveToTable(df_ethnicity_demos, table_demo)\n","    \n","    # Return dictionary with states conversions\n","    return formDictConverter(table_demo, 'demo_group', 'demoID')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f9ca3bd7-6461-4592-a45e-c653c1630b13","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Add all demos to database\n","demosToID = addDemographics()\n","\n","# Add in uppercase versions of 'Female' and 'Male'\n","demosToID['Female'] = demosToID['female']\n","demosToID['Male'] = demosToID['male']"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"df905307-2ff9-43ac-b6fe-49b4d1d14668","showTitle":false,"title":""}},"source":["## Part 3: Populate `NHAINESStat` Database"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"13a39551-9ea9-466a-b4be-1da2cb3c0c19","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Helper function: rename tables\n","def renameCols(df, prev_cols, new_cols):\n","    for i in range(len(prev_cols)):\n","        df = df.withColumnRenamed(prev_cols[i], new_cols[i])\n","    return df\n","\n","# Helper function: drop columns\n","def dropCols(df, drop_cols):\n","    for this_cols in drop_cols:\n","        try:\n","            df = df.drop(col(this_cols))\n","        except:\n","            df = df.drop(this_cols)\n","    return df\n","\n","# Helper function: add columns with all the same values\n","def addCols(df, colNames, addValues):\n","    for i in range(len(colNames)):\n","        df = df.withColumn(colNames[i], lit(addValues[i]))\n","    return df\n","\n","# Helper function: convert demo to demoID\n","def catToID(df, merge_val, cat_cols, dictionary):\n","    df_replace = df.select(cat_cols + merge_val).toPandas()\n","    for cat in cat_cols:\n","        df_replace =  df_replace.replace({cat: dictionary})\n","        try:\n","            df = df.drop(col(cat))\n","        except:\n","            df = df.drop(cat)\n","    df_replace = spark.createDataFrame(df_replace)\n","    df = df.join(df_replace, on=merge_val)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"94f6a365-8c35-404c-a28b-fb3dcae46aee","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Get diabetes prevalence demographics\n","df_nhaines = readInFile('/mnt/jacklynn/nhaines/U.S. NHANES Survey Data.csv')\n","\n","# Rename column names\n","orig_cols = ['SurveyID', 'SystolicBP', 'DiastolicBP', '12drinksInaYear', 'BeenDiagnostedHypertenisve',\n","            'MultipleHypertensionDiagnosis', 'DiagnosedDiabetic', 'DiagnosedPrediabetic',\n","            'DiagnosedAtRiskDiabetes', 'NumMealsNotAtHomePerMonth', 'FamilyMonthlyPovertLevel', \n","            'Smoked100cigs', 'Height', 'Weight', 'Gender', 'Age(yrs)', 'Ethnicity', 'AdultEducationLevel',\n","            'AnnualHouseholdIncome', 'BMI']\n","new_cols = ['nhainesID', 'sbp', 'dbp', 'drinkerID', 'hypertensive', 'multihypertensive', 'diabetes', \n","           'prediabetes', 'diabetesRisk', 'mealsAtHome', 'familyMonthlyPoverty', 'smokerID', 'height',\n","           'weight', 'sexID', 'age', 'ethnicityID', 'educationID', 'incomeID', 'bmi']\n","df_nhaines = renameCols(df_nhaines, orig_cols, new_cols)\n","\n","# Drop columns that aren't needed\n","drop_cols = ['_c0', 'DrinksInLastYear', 'CurrentSmoker']\n","df_nhaines = dropCols(df_nhaines, drop_cols)\n","\n","# Fix drinkerID column\n","drinkerDict = dict()\n","drinkerDict[True] = demosToID['12+ alc. drinks/yr']\n","drinkerDict[False] = demosToID['<12 alc. drinks/yr']\n","df_nhaines = catToID(df_nhaines, ['nhainesID'], ['drinkerID'], drinkerDict)\n","\n","# Fix smokerID column\n","smokerDict = dict()\n","smokerDict[True] = demosToID['Smoked 100+ cigs.']\n","smokerDict[False] = demosToID['Smoked <100 cigs.']\n","df_nhaines = catToID(df_nhaines, ['nhainesID'], ['smokerID'], smokerDict)\n","\n","# Fix sexID column\n","df_nhaines = catToID(df_nhaines, ['nhainesID'], ['sexID', 'incomeID', 'educationID', 'ethnicityID'], demosToID)\n","\n","# Save file\n","saveToTable(df_nhaines, 'dbo.NHAINESStat', change='overwrite')"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"NHAINESStat-Populator","notebookOrigID":1123985657305637,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
