{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c22d2fff-ea22-4ebf-be7b-f29749858350","showTitle":false,"title":""}},"outputs":[],"source":["# Imports\n","from confluent_kafka import Consumer\n","from time import sleep\n","import uuid\n","from confluent_kafka import Producer, Consumer, KafkaError, KafkaException\n","import json\n","from confluent_kafka.admin import AdminClient, NewTopic\n","from pyspark.sql.functions import when\n","from pyspark.sql.functions import col\n","import random as r\n","\n","# Get config\n","from config import user\n","from config import password"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a96f2fbb-8c6a-4f33-9fbc-400d9bb5662a","showTitle":false,"title":""}},"outputs":[],"source":["# Mount point through Oauth security.\n","storageAccount = \"gen10datafund2205\"\n","storageContainer = \"group5container\"\n","clientSecret = \"-ZS8Q~NwOKfwEpVOg3Teb1pPtxDbz616XjlXLbuU\"\n","clientid = \"2ca50102-5717-4373-b796-39d06568588d\"\n","mount_point = \"/mnt/group5/cgm\" \n","\n","configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n","       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n","       \"fs.azure.account.oauth2.client.id\": clientid,\n","       \"fs.azure.account.oauth2.client.secret\": clientSecret,\n","       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n","       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n","\n","# Unmount if exists\n","try: \n","    dbutils.fs.unmount(mount_point)\n","except:\n","    pass\n","\n","# Mount to database\n","dbutils.fs.mount(\n","    source = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\n","    mount_point = mount_point,\n","    extra_configs = configs)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"05e92a40-cdbe-4fef-8df6-aee0be09c1b6","showTitle":false,"title":""}},"outputs":[],"source":["# Get data\n","df = spark.read.options(\n","    inferSchema='True',\n","    delimiter=',',\n","    header='True'\n","    ).csv('/mnt/group5/cgm/CGM_Data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e94fb66e-614e-45af-933d-c772b76dbda3","showTitle":false,"title":""}},"outputs":[],"source":["patient1 = df.filter(df.PtID == 1).sort('DeviceDtTm')\n","patient1 = patient1.filter(patient1['DeviceDtTm'] >= '2000-06-09 00:00:00')\n","\n","patient2 = df.filter(df.PtID == 14).sort('DeviceDtTm')\n","patient2 = patient2.filter(patient2['DeviceDtTm'] >= '2000-03-26 00:00:00')\n","\n","patient3 = df.filter(df.PtID == 33).sort('DeviceDtTm')\n","patient3 = patient3.filter(patient3['DeviceDtTm'] >= '2000-05-14 00:00:00')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"10a4e73e-e5db-405d-9d51-423ff12c87d8","showTitle":false,"title":""}},"outputs":[],"source":["#KAFKA variables, Move to the OS variables or configuration\n","confluentClusterName = \"stage3talent\"\n","confluentBootstrapServers = \"pkc-ldvmy.centralus.azure.confluent.cloud:9092\"\n","confluentTopicName = \"group5cgm\"\n","schemaRegistryUrl = \"https://psrc-gq7pv.westus2.azure.confluent.cloud\"\n","confluentApiKey = \"YHMHG7E54LJA55XZ\"\n","confluentSecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\n","confluentRegistryApiKey = \"YHMHG7E54LJA55XZ\"\n","confluentRegistrySecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\""]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2225c0c2-7413-4d2e-9eb7-99975f98248c","showTitle":false,"title":""}},"outputs":[],"source":["# Get error message functions\n","def error_cb(err):\n","    \"\"\" The error callback is used for generic client errors. These\n","        errors are generally to be considered informational as the client will\n","        automatically try to recover from all errors, and no extra action\n","        is typically required by the application.\n","        For this example however, we terminate the application if the client\n","        is unable to connect to any broker (_ALL_BROKERS_DOWN) and on\n","        authentication errors (_AUTHENTICATION). \"\"\"\n","\n","    print(\"Client error: {}\".format(err))\n","    if err.code() == KafkaError._ALL_BROKERS_DOWN or \\\n","       err.code() == KafkaError._AUTHENTICATION:\n","        # Any exception raised from this callback will be re-raised from the\n","        # triggering flush() or poll() call.\n","        raise KafkaException(err)\n","\n","\n","def acked(err, msg):\n","    \"\"\" \n","        Error callback is used for generic issues for producer errors. \n","        \n","        Parameters:\n","            err (err): Error flag.\n","            msg (str): Error message that was part of the callback.\n","    \"\"\"\n","    if err is not None:\n","        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n","    else:\n","        print(\"Message produced: %s\" % (str(msg)))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7952836f-ee29-4a3b-ad6b-2a2d28f64872","showTitle":false,"title":""}},"outputs":[],"source":["# Create producer\n","\n","#Kakfa Class Setup.\n","p = Producer({\n","    'bootstrap.servers': confluentBootstrapServers,\n","    'sasl.mechanism': 'PLAIN',\n","    'security.protocol': 'SASL_SSL',\n","    'sasl.username': confluentApiKey,\n","    'sasl.password': confluentSecret,\n","    'group.id': str(1),  # this will create a new consumer group on each invocation.\n","    'auto.offset.reset': 'earliest',\n","    'error_cb': error_cb,\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a10be07c-28fb-4e5b-9519-3a6c6d7774fb","showTitle":false,"title":""}},"outputs":[],"source":["# Stream data\n","keep_going = True\n","j = 0\n","for i in range(288):\n","    \n","    # Create dictionary object with that data row\n","    datum1 = dict()\n","    datum2 = dict()\n","    datum3 = dict()\n","    datum = []\n","    \n","    for header in patient1.columns:\n","        datum1[header] = patient1.collect()[j][header]\n","    datum1['DeviceDtTm'] = datum1['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    for header in patient2.columns:\n","        datum2[header] = patient2.collect()[j][header]\n","    datum2['DeviceDtTm'] = datum2['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    for header in patient3.columns:\n","        datum3[header] = patient3.collect()[j][header]\n","    datum3['DeviceDtTm'] = datum3['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    datum.append(datum1)\n","    datum.append(datum2)\n","    datum.append(datum3)\n","    \n","    # Produce and flush data\n","    p.produce(confluentTopicName, json.dumps(datum))\n","    p.flush()\n","    \n","    # Print flush info\n","    print(f'Current step: {j}\\n({datum})\\n')\n","    j += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"02aa62dd-2261-4eac-9b4f-d452c5ef18ab","showTitle":false,"title":""}},"outputs":[],"source":["patient4 = df.filter(df.PtID == 1).sort('DeviceDtTm')\n","patient4 = patient4.filter(patient4['DeviceDtTm'] >= '2000-06-10 00:00:00')\n","\n","patient5 = df.filter(df.PtID == 14).sort('DeviceDtTm')\n","patient5 = patient5.filter(patient5['DeviceDtTm'] >= '2000-03-27 00:00:00')\n","\n","patient6 = df.filter(df.PtID == 33).sort('DeviceDtTm')\n","patient6 = patient6.filter(patient6['DeviceDtTm'] >= '2000-05-15 00:00:00')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"88af59b0-448c-49cd-892b-1675f097de6f","showTitle":false,"title":""}},"outputs":[],"source":["# Stream data\n","keep_going = True\n","j = 0\n","for i in range(288):\n","    \n","    # Create dictionary object with that data row\n","    datum4 = dict()\n","    datum5 = dict()\n","    datum6 = dict()\n","    datum = []\n","    \n","    for header in patient4.columns:\n","        datum4[header] = patient4.collect()[j][header]\n","    datum4['DeviceDtTm'] = datum4['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    for header in patient5.columns:\n","        datum5[header] = patient5.collect()[j][header]\n","    datum5['DeviceDtTm'] = datum5['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    for header in patient6.columns:\n","        datum6[header] = patient6.collect()[j][header]\n","    datum6['DeviceDtTm'] = datum6['DeviceDtTm'].strftime('%m/%d/%Y, %H:%M:%S')\n","    \n","    datum.append(datum4)\n","    datum.append(datum5)\n","    datum.append(datum6)\n","    \n","    # Produce and flush data\n","    p.produce(confluentTopicName, json.dumps(datum))\n","    p.flush()\n","    \n","    # Print flush info\n","    print(f'Current step: {j}\\n({datum})\\n')\n","    j += 1\n","    sleep(10) # Sleep for 10 seconds"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"cgm_producer","notebookOrigID":2451669969096220,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
